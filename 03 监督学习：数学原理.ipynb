{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 监督学习：数学原理\n",
    "## 3.1 监督学习\n",
    "监督学习是处理 $y = F(x)$ 中 $y$ 已知，求解功能函数 $F$ 的问题。根据 $y$ 的差异划分为：\n",
    "- 回归：$y$ 是连续值或不可枚举的离散值。\n",
    "- 分类：$y$ 是可枚举值的离散值。\n",
    "\n",
    "多数监督学习算法都同时支持处理回归&分类问题。\n",
    "## 3.2 回归\n",
    "### 3.2.1 损失函数\n",
    "**平均偏差误差**：$ MBE = \\frac{1}{n}\\sum_{i = 1}^n(y_i - \\hat y_i) $ \n",
    "- 缺点：受到误差方向的迎向，正负抵，精确度不足。\n",
    "\n",
    "**平均绝对值误差（L1 loss）**：$ MAE = \\frac{1}{n}\\sum_{i = 1}^n|y_i - \\hat y_i| $\n",
    "- 优点：鲁棒性好，少数异常点对终值影响较小。\n",
    "- 缺点：零点不可导，可能存在多解；损失值偏差较小的与偏差较大的共享梯度，需要使用可变的学习率优化。\n",
    "\n",
    "**均方根误差（L2 loss）**：$ RMSE = \\sqrt {MSE} = \\sqrt {\\frac{1}{n}\\sum_{i = 1}^n(y_i - \\hat y_i)^2} $\n",
    "- 优点：函数处处可导，有相对稳定的解。\n",
    "- 缺点：异常值（误差超过1）敏感，会牺牲正常点的预测效果以优化权重更高的离群点，可用导数阈值优化。\n",
    "\n",
    "**平滑绝对误差**：$ Huber loss = \\sum_{i=1}^n \\begin{cases} 0.5*(y_i - \\hat y_i)^2 & |y_i - \\hat y_i|<\\delta \\\\ \\delta * (|y_i - \\hat y_i|-0.5 \\delta ^2) & else \\end{cases}$\n",
    "- 小于 $\\delta$ 为L2范数，其余为L1范数，让预测值与真实值差别过大时，梯度值不至于过大，反之又足够小。\n",
    "- smooth L1($\\delta = 1$) $ = \\sum_{i=1}^n \\begin{cases} 0.5*(y_i - \\hat y_i)^2 & |y_i - \\hat y_i|<1 \\\\ |y_i - \\hat y_i|-0.5 & else \\end{cases}$\n",
    "\n",
    "**Log-Cosh Loss**：$ \\sum_{i=1}^n log(cosh(y_i - \\hat y_i)) $\n",
    "- 对于较小的$x$，$log(cosh(x))$近似于$\\frac{(x^2)}{2}$，对于较大的$x$，近似于$|x|-log(2)$。\n",
    "- 优点：具有Huber loss所有的优点，预测误差的双曲余弦对数；优势是二阶导数处处可微。\n",
    "- 缺点：误差极大时，一阶梯度和Hessian会变成定值，导致如XGBoost出现缺少分裂点的情况。\n",
    "\n",
    "**分位数损失**：$ Quantile Loss = \\sum_{y_i<\\hat y_i}(1-\\lambda)|y_i-\\hat y_i| + \\sum_{y_i\\geq\\hat y_i}\\lambda|y_i - \\hat y_i|, \\lambda \\in [0,1] $\n",
    "- 面对不太线性的现实问题中，偏向针对某一区间预测的预测效果的解决方案。\n",
    "- 基于对正误差和反误差的重视程度，通过$\\lambda$（分位值）对偏高预测值与偏低预测值给予不同的惩罚，让函数对中值产生偏移。\n",
    "\n",
    "以上只是回归问题常用的损失函数，当然还有许多损失函数未提及，也可以根据实际问题自定义损失函数。\n",
    "### 3.2.2 寻解算法：最小二乘法（OLS）\n",
    "**最小二乘法（最小平方法）**：通过最小化误差的平方和寻找数据的最佳匹配函数。对于求解最小二乘法结果的方法有**矩阵解法**和**梯度下降法**。\n",
    "\n",
    "*以前我以为最小二乘法和梯度下降法是两个方法，其实梯度下降法只是最小二乘法求解的方法之一。*\n",
    "#### 3.2.2.1 矩阵解法\n",
    "为方便后续讲解，做出如下前提假设：\n",
    "- **模型假设**：$ F(x)=a_1*x_1+a_2*x_2+……+a_n*x_n+b=[x_1,x_2,……,x_n,1][a_1,a_2,……,a_n,b]^T=XA $\n",
    "- **损失函数**：$ L(x,a)=\\frac{1}{2n}\\sum_{i=1}^n[(a_1*x_{i1}+a_2*x_{i2}+……+a_n*x_{in}+b)-y_i]^2=\\frac{1}{2n}(XA-Y)^T(XA-Y) $\n",
    "\n",
    "依据假设，当损失函数的一阶偏导数（$\\partial A$）为为零时，损失函数取最小值，即：\n",
    "$$ \\frac{\\partial L}{\\partial A}=\\frac{\\partial L}{\\partial a_1}+\\frac{\\partial L}{\\partial a_2}+……+\\frac{\\partial L}{\\partial a_n}=\\frac{1}{n}\\sum_{i=1}^n(a_1x_{i1}+a_2x_{i2}+……+a_nx_{in})*(x_{i1}+x_{i2}+……+x_{in})=\\frac{X^T(XA-Y)}{n}=0 $$\n",
    "求解：\n",
    "$$ A = (X^TX)^{-1}X^TY $$\n",
    "以上的推导过程表明矩阵解法有如下限制：\n",
    "- 1.系数矩阵$A$必须存在逆矩阵（可舍弃边缘特征使矩阵可逆缓解）。\n",
    "- 2.系数矩阵$A$的求逆过程的耗时受矩阵规模影响（利用降维处理缓解）。\n",
    "- 3.模型假设若非线性，求解将十分复杂或难以开展（可以转化为线性关系缓解）。\n",
    "\n",
    "*（）中是一些预处理方案，优化矩阵解法的适配问题。*\n",
    "#### 3.2.2.2 梯度下降法\n",
    "梯度下降法是利用梯度逐步迭代模型参数，使损失函数达到最小值的方法。举例如下：\n",
    "```\n",
    "想象你和贝爷一起进行丛林探险，露宿山崖边缘，晨间丛林里雾气弥漫，你们选择下至山下的峡谷补充水源。此时，你们**各选了一个位置**开始下山，你用脚一点点感受被踩踏的岩石，选择合适的落脚点，你总是**优先选择垂直向下（即坡度最陡）**的方向下山，重复如此，最终你们下到了山脚。\n",
    "```\n",
    "梯度下降法的过程与之相似，使用相同的假设表达如下：\n",
    "- **模型假设**：$ F(x)=XA $ 并随机初始化$A$中参数。\n",
    "- **损失函数**：$ L(x,a)=\\frac{1}{2n}(XA-Y)^T(XA-Y) $。\n",
    "- **偏导数**：$ \\frac{\\partial L}{\\partial A}=\\frac{1}{n}X^T(XA-Y) $。\n",
    "- **梯度下降**：$ A=A-\\eta \\frac{\\partial L}{\\partial A},\\eta \\in (0,1) $。\n",
    "- **终止条件**：梯度值为零或不可改善时，则达到损失函数最小值，此时的模型参数为最优参数。\n",
    "\n",
    "学习率（$\\eta$）与梯度（$\\frac{\\partial L}{\\partial A}$）共同决定每轮参数矩阵迭代的步长，其中学习率是用户设定值，需注意：\n",
    "- 模型训练前应特征缩，提高$\\eta$作用效果，高效下降。\n",
    "- $\\eta$设定过小，耗时严重；设定过大，反复跳跃难收敛。\n",
    "\n",
    "与矩阵的解法中求解一阶导数为零不同，由于训练数据$X$，随机参数$A$、学习率$\\eta$均为已知项，其目标是将现有的随机参数不断改善成最优函数的接近值，所以突破了矩阵解法的限制，更加通用。但是梯度下降法的主要挑战是复杂的损失函数，除了全局最小值外，还存在多个局部最小值，有时可能永远到不了全局最小值。\n",
    "在实际使用梯度下降法时，由于计算梯度方式的差异，衍生出三种方法：\n",
    "- **批梯度下降（BGD）**：每轮迭代计算梯度时使用全部训练数据；优点是朝着最小值迭代，缺点是样本值很大时每轮更新速度会很慢。\n",
    "- **随机梯度下降（SGD）**：每轮迭代计算梯度时随机抽取一条训练数据；优点是每轮更新速度快，缺点是样本噪点较多时未必朝着极小值方向更新（多轮迭代能保障整体大致朝着极小值方向更新）。\n",
    "- **小批量梯度下降（MBGD）**：每轮迭代计算梯度时随机抽取一定比例训练数据；优点是平衡更新梯度方向准确性与更新速度，缺点是batch值较难确定。\n",
    "\n",
    "虽然随机性可以很好的跳过局部最优值，所以比较推荐使用小批量梯度下降法。由于随机梯度下降较难能达到最小值，可使用逐渐降低学习率缓和这一问题，这个过程被称为模拟退火。\n",
    "\n",
    "以上虽然只使用了相对简单的假设进行举例，但相同的梯度下降法可以推广到更高维，更复杂的损失函数中使用。\n",
    "## 3.3 分类\n",
    "分类问题常见的是二分类问题，比如是否是垃圾邮件，是否会点击广告、是否是欺诈用户等；但也可以拥有任意数量的类，称作多类分类问题。\n",
    "根据处理问题的不同，前者使用二分类器，后者为多类分类器（也被叫做多项式分类器）；一些算法（比如 SVM 分类器或者线性分类器）是严格的二分类器，另一些算法（比如随机森林分类器或者朴素贝叶斯分类器）同时支持；故有许多策略可以让二分类器去处理多类分类问题。\n",
    "- **一对所有（OvA）**：创建与类别数量$N$相等的分类器个数，每个二分类器训练时只设定其中一个类别为正例，其余均为负例，选出决策分数最高的分类器所对应的类。\n",
    "- **一对一（OvO）**：将类别数量$N$两两配对作为训练子集单独训练二分类器，共计训练$\\frac{N*(N-1)}{2}$个分类器，选出全部分类器中类别胜出最多的类。\n",
    "\n",
    "参考资料：\n",
    "- [Sklearn 与 TensorFlow 机器学习实用指南](https://www.bookstack.cn/books/hands_on_Ml_with_Sklearn_and_TF)\n",
    "- [最小二乘法](https://zhuanlan.zhihu.com/p/38128785)\n",
    "- [波士顿房价预测任务](https://www.paddlepaddle.org.cn/tutorials/projectdetail/1515038#anchor-13)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}