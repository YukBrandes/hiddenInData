{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 监督学习\n",
    "## 3.1 分类 or 回归\n",
    "监督学习是处理 $y = F(x)$ 中 $y$ 已知，求解功能函数 $F$ 的问题。根据 $y$ 的差异划分为：\n",
    "- 回归：$y$ 是连续值或不可枚举的离散值。\n",
    "- 分类：$y$ 是可枚举值的离散值。\n",
    "\n",
    "多数监督学习算法都同时支持处理回归&分类问题。\n",
    "## 3.2 回归问题的数学原理\n",
    "### 3.2.1 损失函数\n",
    "**平均偏差误差**：$ MBE = \\frac{1}{n}\\sum_{i = 1}^n(y_i - \\hat y_i) $ \n",
    "- 缺点：受到误差方向的迎向，正负抵，精确度不足。\n",
    "\n",
    "**平均绝对值误差（L1 loss）**：$ MAE = \\frac{1}{n}\\sum_{i = 1}^n|y_i - \\hat y_i| $\n",
    "- 优点：鲁棒性好，少数异常点对终值影响较小。\n",
    "- 缺点：零点不可导，可能存在多解；损失值偏差较小的与偏差较大的共享梯度，需要使用可变的学习率优化。\n",
    "\n",
    "**均方根误差（L2 loss）**：$ RMSE = \\sqrt {MSE} = \\sqrt {\\frac{1}{n}\\sum_{i = 1}^n(y_i - \\hat y_i)^2} $\n",
    "- 优点：函数处处可导，有相对稳定的解。\n",
    "- 缺点：异常值（误差超过1）敏感，会牺牲正常点的预测效果以优化权重更高的离群点，可用导数阈值优化。\n",
    "\n",
    "**平滑绝对误差**：$ Huber loss = \\sum_{i=1}^n \\begin{cases} 0.5*(y_i - \\hat y_i)^2 & |y_i - \\hat y_i|<\\delta \\\\ \\delta * (|y_i - \\hat y_i|-0.5 \\delta ^2) & else \\end{cases}$\n",
    "- 小于 $\\delta$ 为L2范数，其余为L1范数，让预测值与真实值差别过大时，梯度值不至于过大，反之又足够小。\n",
    "- smooth L1($\\delta = 1$) $ = \\sum_{i=1}^n \\begin{cases} 0.5*(y_i - \\hat y_i)^2 & |y_i - \\hat y_i|<1 \\\\ |y_i - \\hat y_i|-0.5 & else \\end{cases}$\n",
    "\n",
    "**Log-Cosh Loss**：$ \\sum_{i=1}^n log(cosh(y_i - \\hat y_i)) $\n",
    "- 对于较小的$x$，$log(cosh(x))$近似于$\\frac{(x^2)}{2}$，对于较大的$x$，近似于$|x|-log(2)$。\n",
    "- 优点：具有Huber loss所有的优点，预测误差的双曲余弦对数；优势是二阶导数处处可微。\n",
    "- 缺点：误差极大时，一阶梯度和Hessian会变成定值，导致如XGBoost出现缺少分裂点的情况。\n",
    "\n",
    "**分位数损失**：$ Quantile Loss = \\sum_{y_i<\\hat y_i}(1-\\lambda)|y_i-\\hat y_i| + \\sum_{y_i\\geq\\hat y_i}\\lambda|y_i - \\hat y_i|, \\lambda \\in [0,1] $\n",
    "- 面对不太线性的现实问题中，偏向针对某一区间预测的预测效果的解决方案。\n",
    "- 基于对正误差和反误差的重视程度，通过$\\lambda$（分位值）对偏高预测值与偏低预测值给予不同的惩罚，让函数对中值产生偏移。\n",
    "\n",
    "以上只是回归问题常用的损失函数，当然还有许多损失函数未提及，也可以根据实际问题自定义损失函数。\n",
    "### 3.2.2 寻解算法\n",
    "#### 3.2.2.1 最小二乘法\n",
    "1. 计算逆矩阵，如果逆矩阵不存在，可以通过对样本数据进行整理，去掉冗余特征，让矩阵的行列式不为0。\n",
    "2. 当样本特征矩阵非常的大的时候，计算逆矩阵是一个非常耗时的工作，甚至不可行，此时以主成分分析降低特征的维度。\n",
    "3. 拟合函数不是线性的，需要通过一些技巧转化为线性才能使用。\n",
    "\n",
    "#### 3.2.2.2 梯度下降\n",
    "由于最小二乘法受限于线性拟合函数、逆矩阵存在且求解过程占用大量计算资源等问题，本段阐述的梯度下降将有效解决上述问题。\n",
    "\n",
    "#### 3.2.2.3 随机梯度下降\n",
    "随机梯度下降给我的感觉有点像“最速曲线”的概念。\n",
    "\n",
    "以上虽然只是使用的二维空间进行的举例，但相同的原理可以推广到高维，使得复杂的模型可以使用这些寻解算法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 分割线，下面还没梳理"
   ]
  },
  {
   "source": [
    "### 3.2.3 过拟合\n",
    "## 3.3 分类问题的数学原理\n",
    "## 3.4 模型假设\n",
    "### 3.4.1 线性模型\n",
    "### 3.4.2 线性和二次判别分析\n",
    "### 3.4.3 内核岭回归\n",
    "### 3.4.4 支持向量机\n",
    "### 3.4.5 随机梯度下降法\n",
    "### 3.4.6 最近的邻居\n",
    "### 3.4.7 高斯过程\n",
    "### 3.4.8 横向分解\n",
    "### 3.4.9 朴素贝叶斯\n",
    "### 3.4.10 决策树\n",
    "### 3.4.11 整体方法\n",
    "### 3.4.12 多类和多输出算法\n",
    "### 3.4.13 特征选择\n",
    "### 3.4.14 Semi-supervised学习\n",
    "### 3.4.15 等张回归\n",
    "### 3.4.16 概率校准\n",
    "### 3.4.17 神经网络模型(监督)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}