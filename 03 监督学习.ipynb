{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 监督学习\n",
    "## 3.1 现实问题：分类 or 回归\n",
    "监督学习处理的是含有数据标签的问题，即 $y = F(x)$ 中，建模时 $y$ 是已知项；那么根据 $y$ 的差异，又可以将其划分：\n",
    "- 回归：目标变量 $y$ 是连续值或数量不可枚举值。\n",
    "- 分类：目标变量 $y$ 是离散且数量可枚举值。\n",
    "\n",
    "除个别算法外，多数算法都支持处理上述两类问题，后续不分开此两类阐述。\n",
    "## 3.2 损失函数\n",
    "正如前文所提，损失函数是机器学习的重要组成部分，故于此处优先独立探讨，阐述基本方法。\n",
    "### 3.2.1 偏差&方差\n",
    "**偏差**：描述的是预测值的期望与真实值之间的差距；偏差越大，越偏离真实数据。\n",
    "\n",
    "**方差**：描述的是预测值的变化范围，离散程度（即离期望值的距离）；方差越大，数据的分布越分散。\n",
    "\n",
    "建模的目标便是构建低偏差和低方差的模型，但是基于训练数据集的低偏差、低方差建模可能会带来机器学习的一个常见问题：**过拟合**（反之则为“欠拟合”）。模型完美诠释训练数据时，大多很难有效推广至测试数据，仅学习个性，未捕捉典型规律，故，损失函数同时作用训练集和测试集，当损失函数达到两边接近的稳定状态后，模型便停止优化。此外，解决过拟合的一般方法：一是，**扩充训练数据**；二是，**正则化**。\n",
    "### 3.2.2 回归\n",
    "\n",
    "均方误差\n",
    "[最小二乘法](https://zhuanlan.zhihu.com/p/38128785)：偏导数为0\n",
    "首先，最小二乘法需要计算 [公式] 的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让 [公式] 的行列式不为0，然后继续使用最小二乘法。\n",
    "\n",
    "第二，当样本特征 [公式] 非常的大的时候，计算 [公式] 的逆矩阵是一个非常耗时的工作（ [公式] 的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个 [公式] 到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。\n",
    "\n",
    "第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。\n",
    "\n",
    "梯度下降\n",
    "\n",
    "\n",
    "### 3.2.3 分类\n",
    "\n",
    "\n",
    "### 3.2.4 正则化\n",
    "\n",
    "### 3.2.5 寻解算法\n",
    "\n",
    "相同的原理可以推广到高维，虽然超过三维的东西很难可视化。\n",
    "\n",
    "\n",
    "## 3.1 线性模型\n",
    "\n",
    "\n",
    "\n",
    "对于一个像这样的简单问题，我们可以使用微积分计算闭式解，来寻找最佳的β参数，它使我们的损失函数最小。但是随着成本函数的复杂性的增长，使用微积分寻找闭式解就不太可能了。这就推动了一种迭代方式，叫做梯度下降，它允许我们使复杂的损失函数最小。\n",
    "\n",
    "梯度下降\n",
    "\n",
    "\n",
    "## 3.2 线性和二次判别分析\n",
    "## 3.3 内核岭回归\n",
    "## 3.4 支持向量机\n",
    "## 3.5 随机梯度下降法\n",
    "## 3.6 最近的邻居\n",
    "## 3.7 高斯过程\n",
    "## 3.8 横向分解\n",
    "## 3.9 朴素贝叶斯\n",
    "## 3.10 决策树\n",
    "## 3.11 整体方法\n",
    "## 3.12 多类和多输出算法\n",
    "## 3.13 特征选择\n",
    "## 3.14 Semi-supervised学习\n",
    "## 3.15 等张回归\n",
    "## 3.16 概率校准\n",
    "## 3.17 神经网络模型(监督)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "回归：预测连续值\n",
    "所以我们如何解决这些问题？\n",
    "线性回归（普通最小二乘）\n",
    "梯度下降：习得参数\n",
    "过拟合\n",
    "这就完成了\n",
    "练习材料和扩展阅读\n",
    "2.1a 线性回归\n",
    "2.1b 实现梯度下降"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}