{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd09b02d228b0deb9bd5deb88192194232067fb7f0761ef8eded4fa6ac3b1d9be68",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 4 sklearn监督学习\n",
    "预加载实验所需的回归、分类数据及预定义功能函数："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error\n",
    "\n",
    "# 自定义函数\n",
    "## 评估回归\n",
    "def EvaluateReg(x_train,x_test,y_train,y_test,model):\n",
    "    pre_train = model.predict(x_train)\n",
    "    pre_test = model.predict(x_test)\n",
    "    mse_train = round(mean_squared_error(y_train,pre_train),2)\n",
    "    mse_test = round(mean_squared_error(y_test,pre_test),2)\n",
    "    r2_train = round(model.score(x_train,y_train),2)\n",
    "    r2_test = round(model.score(x_test,y_test),2)\n",
    "    print('train_r2:%s,train_mse:%s,\\ntest_r2:%s,test_mse:%s.' % (r2_train,mse_train,r2_test,mse_test))\n",
    "\n",
    "## 评估分类\n",
    "def EvaluateClass(x_train,x_test,y_train,y_test,model):\n",
    "    pre_train = model.predict(x_train)\n",
    "    pre_test = model.predict(x_test)\n",
    "    roc_train = round(roc_auc_score(y_train,pre_train),2)\n",
    "    roc_test = round(roc_auc_score(y_test,pre_test),2)\n",
    "    accraucy_train = round(model.score(x_train,y_train),2)\n",
    "    accraucy_test = round(model.score(x_test,y_test),2)\n",
    "    print('train_accraucy:%s,train_auc:%s,\\ntest_accraucy:%s,test_auc:%s.' % (accraucy_train,roc_train,accraucy_test,roc_test))\n",
    "\n",
    "# 实验数据\n",
    "## 回归数据\n",
    "dt1 = pd.read_csv('data/pp_gas_emission/gt_2011.csv')\n",
    "dt2 = pd.read_csv('data/pp_gas_emission/gt_2012.csv')\n",
    "dt3 = pd.read_csv('data/pp_gas_emission/gt_2013.csv')\n",
    "dt4 = pd.read_csv('data/pp_gas_emission/gt_2014.csv')\n",
    "train = pd.concat([dt1,dt2,dt3,dt4],axis=0).reset_index(drop=True)\n",
    "test = pd.read_csv('data/pp_gas_emission/gt_2015.csv')\n",
    "x_train1,y_train1,x_test1,y_test1 = train.iloc[:,:9],np.ravel(train.NOX),test.iloc[:,:9],np.ravel(test.NOX)\n",
    "## 分类数据\n",
    "data = pd.read_csv('data/default of credit card clients.csv')\n",
    "x_train2,x_test2,y_train2,y_test2 = train_test_split(data.iloc[:,1:],data['default payment next month'],test_size=0.3,random_state=11)\n",
    "y_train2,y_test2 = np.ravel(y_train2),np.ravel(y_test2)\n",
    "\n",
    "# 数据预处理\n",
    "## 回归数据\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train1)\n",
    "x_train1 = scaler.transform(x_train1)\n",
    "x_test1 = scaler.transform(x_test1)\n",
    "## 分类数据\n",
    "scaler.fit(x_train2)\n",
    "x_train2 = scaler.transform(x_train2)\n",
    "x_test2 = scaler.transform(x_test2)"
   ]
  },
  {
   "source": [
    "## 4.1 广义线性模型\n",
    "### 4.1.1 普通最小二乘法\n",
    "处理[回归问题](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)，对特征有**非奇异性**（满秩、特征比数多）与**中心化/标准化**要求。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.68,\ntest_r2:0.26,test_mse:91.84.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,reg)"
   ]
  },
  {
   "source": [
    "### 4.1.2 岭回归与分类\n",
    "#### 4.1.2.1 岭回归\n",
    "岭回归通过对系数的大小施加惩罚来解决普通最小二乘法对共线性敏感的问题，其最小化的是带L2正则化的残差平方和，L2正则系数 $\\alpha$ 值越大，收缩量越大，对共线性的鲁棒性也越强。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.68,\ntest_r2:0.26,test_mse:91.85.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "ridge = linear_model.Ridge(alpha=0.1, # 数值越大，惩罚越大\n",
    "                           tol=0.001, # 预测精度\n",
    "                           solver='auto', # auto,svd,cholesky,lsqr,sparse_cg,sag,saga\n",
    "                           max_iter=None, # 共轭函数求解迭代次数,sparse_cg and lsqr\n",
    "                           random_state=None) # 控制sag、saga\n",
    "ridge.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,ridge)"
   ]
  },
  {
   "source": [
    "由于岭回归对共线性的改善，所以可以使用岭迹图来判断是否剔除某参数以避免共线性。此外，ridge提供了便捷的交叉验证建模函数，进行快速定位："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.71,\ntest_r2:0.25,test_mse:92.7.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9.5"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ridgeCV = linear_model.RidgeCV(alphas=np.arange(0,10,0.5),\n",
    "                               cv=5, # None：留一交叉验证，int：指定折叠数\n",
    "                               scoring='r2') # 评估函数\n",
    "ridgeCV.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,ridgeCV)\n",
    "ridgeCV.alpha_ # ridgeCV.best_score_,ridgeCV.coef_,ridgeCV.intercept_"
   ]
  },
  {
   "source": [
    "#### 4.1.2.2 岭分类\n",
    "适用于[二分类问题](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)，模型将类别转换为 $-1,1$ 两种标签，然后使用回归的方式计算。                                              "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.69,train_auc:0.67,\ntest_accraucy:0.68,test_auc:0.67.\n"
     ]
    }
   ],
   "source": [
    "ridgeClassiflier = linear_model.RidgeClassifier(alpha=0.1,\n",
    "                                                tol=0.01,\n",
    "                                                class_weight='balanced') # dict,balanced\n",
    "ridgeClassiflier.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,ridgeClassiflier)"
   ]
  },
  {
   "source": [
    "也有与之相匹配的快速交叉验证方法："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.69,train_auc:0.67,\ntest_accraucy:0.68,test_auc:0.67.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "ridgeClassiflierCV = linear_model.RidgeClassifierCV(alphas=range(670,680,1),\n",
    "                                                    cv=3,\n",
    "                                                    scoring='roc_auc',\n",
    "                                                    class_weight='balanced')\n",
    "ridgeClassiflierCV.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,ridgeClassiflierCV)\n",
    "ridgeClassiflierCV.alpha_"
   ]
  },
  {
   "source": [
    "### 4.1.3 Lasso\n",
    "Lasso通过最小化L1正则化的残差平方和，快速提取出重要变量，简化模型（使用LASSO回归系数轨迹）。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.76,\ntest_r2:0.25,test_mse:93.21.\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(alpha=0.01, # 数值越大，惩罚越大\n",
    "                           tol=0.01,\n",
    "                           precompute=True) # 启用预定义的格拉姆矩阵加速计算\n",
    "lasso.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.5,train_mse:65.57,\ntest_r2:0.09,test_mse:113.02.\n"
     ]
    }
   ],
   "source": [
    "lassoCV = linear_model.LassoCV(cv=3,\n",
    "                               eps=0.01,\n",
    "                               n_alphas=100, # alphas=[0.1,10]\n",
    "                               tol=0.01)\n",
    "lassoCV.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,lassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.8,\ntest_r2:0.24,test_mse:93.86.\n"
     ]
    }
   ],
   "source": [
    "lassoLarsCV = linear_model.LassoLarsCV(cv=3,\n",
    "                                       max_n_alphas=1000,\n",
    "                                       eps=2.220446049250313e-16)\n",
    "lassoLarsCV.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,lassoLarsCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.51,train_mse:63.68,\ntest_r2:0.26,test_mse:91.84.\n"
     ]
    }
   ],
   "source": [
    "lassoLarsIC = linear_model.LassoLarsIC(criterion='aic') # aic/bic\n",
    "lassoLarsIC.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,lassoLarsIC)"
   ]
  },
  {
   "source": [
    "与岭回归一样，Lasso也可以进行多任务分类：MultiTaskLasso。\n",
    "### 4.1.4 Logistic回归\n",
    "用于处理二分类问题，lbfgs求解器鲁棒性占优；对于大型数据集，saga求解器通常更快。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.81,train_auc:0.61,\ntest_accraucy:0.81,test_auc:0.61.\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=10000, # 正则化系数的倒数\n",
    "                                     #class_weight='balanced',\n",
    "                                     penalty='elasticnet', # l1,l2,elasticnet;l1-liblinear\n",
    "                                     solver='saga', # liblinear(坐标轴下降法),lbfgs(loss二阶导),newton-cg(loss二阶导),sag(随机平均梯度下降),saga\n",
    "                                     tol=0.01, # 迭代终止判据的误差范围\n",
    "                                     max_iter=500,\n",
    "                                     random_state=11, # sag,saga,liblinear时\n",
    "                                     l1_ratio=0.9) # 仅在惩罚='elasticnet'时使用；值为0等同于使用惩罚='l2'，值为1等同于使用惩罚='l1'。当0 < l1_ratio <1时，罚点球是L1和L2的组合。\n",
    "\n",
    "lr.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.72,train_auc:0.61,\ntest_accraucy:0.73,test_auc:0.61.\n"
     ]
    }
   ],
   "source": [
    "lrCV = linear_model.LogisticRegressionCV(Cs=[0.1,1,10,100,1000,10000],\n",
    "                                         cv=3,\n",
    "                                         #class_weight='balanced',\n",
    "                                         penalty='elasticnet', # l1,l2,elasticnet\n",
    "                                         scoring='roc_auc',\n",
    "                                         solver='saga', # newton-cg,lbfgs,liblinear,sag,saga\n",
    "                                         tol=0.01,\n",
    "                                         max_iter=500,\n",
    "                                         random_state=11,\n",
    "                                         l1_ratios=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "lrCV.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,lrCV)"
   ]
  },
  {
   "source": [
    "一点感悟：balanced调参，非balanced训练最终模型，效果比较好。\n",
    "### 4.1.6 随机梯度下降\n",
    "适合大数据集，设定 loss=\"log\" ，则 SGDClassifier 拟合一个逻辑回归模型，而 loss=\"hinge\" 拟合线性支持向量机（SVM）。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.43,train_mse:74.59,\ntest_r2:0.19,test_mse:99.83.\n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDRegressor(loss='epsilon_insensitive', # squared_loss,huber(对异常不敏感),epsilon_insensitive(超限后线性),squared_epsilon_insensitive\n",
    "                                penalty='elasticnet', # l2,l1,elasticnet\n",
    "                                alpha=0.0001, # 值越大，正则化越强;optimal\n",
    "                                l1_ratio=0.5,\n",
    "                                max_iter=100,\n",
    "                                tol=0.01,\n",
    "                                epsilon=0.1, # huber,epsilon_insensitive,squared_epsilon_insensitive 敏感阈值界限\n",
    "                                random_state=11,\n",
    "                                learning_rate='optimal', # constant,optimal,invscaling,adaptive\n",
    "                                eta0=0.1, # 'constant','invscaling','adaptive'的初始化学习率\n",
    "                                # power_t=0.25, # invscaling 所需\n",
    "                                early_stopping=True,\n",
    "                                validation_fraction=0.1, # early_stopping预留验证集比例\n",
    "                                n_iter_no_change=50)\n",
    "sgd.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.81,train_auc:0.68,\ntest_accraucy:0.8,test_auc:0.69.\n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDClassifier(loss='hinge', # hinge,log,modified_huber,squared_hinge,perceptron;squared_loss,huber,epsilon_insensitive,squared_epsilon_insensitive.\n",
    "                                 penalty='elasticnet',\n",
    "                                 alpha=0.2,\n",
    "                                 l1_ratio=0.2,\n",
    "                                 random_state=11,\n",
    "                                 learning_rate='invscaling',\n",
    "                                 eta0=0.1,\n",
    "                                 power_t=0.4,\n",
    "                                 early_stopping=True,\n",
    "                                 validation_fraction=0.1,\n",
    "                                 n_iter_no_change=100,\n",
    "                                 class_weight='balanced')\n",
    "sgd.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,sgd)"
   ]
  },
  {
   "source": [
    "### 4.1.7 多项式回归\n",
    "构建多项式、交互特征，拓展特征矩阵后建模"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.81,train_auc:0.59,\ntest_accraucy:0.8,test_auc:0.58.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('poly',PolynomialFeatures(degree=3,interaction_only=False)),('sgd',linear_model.SGDClassifier(loss='hinge',penalty='elasticnet',alpha=0.2,l1_ratio=0.2))])\n",
    "model.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,model)\n"
   ]
  },
  {
   "source": [
    "### 4.1.8 稳健回归\n",
    "稳健回归（robust regression）特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误；但在高维数据条件下（ n_features大），一般而言很难完成稳健拟合，很可能完全不起作用。\n",
    "## 4.2 线性和二次判别分析\n",
    "线性判别分析（LDA）和二次判别分析（QDA）是有监督的数据降维思想，不同于后面要提及的主成分分析方法（PCA），属于无监督方法；所以LDA与QDA即可降维亦可分类。\n",
    "### 4.2.1. 线性判别分析\n",
    "LDA的思想是将数据投影到低维空间后使得同一类数据尽可能的紧凑，不同类的数据尽可能分散，适用于不同类别间数据均值差异很大，方差差异不大的情景。实际中，即使不满足上述要求，LDA一般也可取得很好的效果。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.81,train_auc:0.64,\ntest_accraucy:0.81,test_auc:0.65.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(solver='eigen', # svd:适合特征多,lsqr,eigen\n",
    "          shrinkage=0.5, # [0,1];lsqr/eigen & covariance_estimator=None\n",
    "          n_components=1) # 降维的维数；用于分类则置None。\n",
    "lda.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,lda)"
   ]
  },
  {
   "source": [
    "### 4.2.2 二次判别分析\n",
    "不同于LDA，QDA利用贝叶斯规则拟合数据的条件密度，生成二次决策边界。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.53,train_auc:0.63,\ntest_accraucy:0.52,test_auc:0.63.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "qda = QDA().fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,qda)"
   ]
  },
  {
   "source": [
    "## 4.3 内核岭回归"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:-0.16,train_auc:0.72,\ntest_accraucy:-0.15,test_auc:0.73.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import kernel_ridge\n",
    "clf = kernel_ridge.KernelRidge(alpha=0.1, # 正则系数\n",
    "                               kernel='linear') # linear,rbf,sigmoid,poly/polynomial,laplacian,cosine,chi2,additive_chi2\n",
    "                               # gamma,仅rbf,laplacian,poly,chi2,sigmoid\n",
    "                               # degree,仅poly\n",
    "                               # coef0,仅poly、sigmoid核\n",
    "clf.fit(x_train1,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,clf)\n"
   ]
  },
  {
   "source": [
    "## 4.4 支持向量机\n",
    "- 高维空间中高效\n",
    "- features大于samples下仍有效\n",
    "- 注意正则和核函数避免过拟合"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_r2:0.62,train_mse:50.07,\ntest_r2:-0.33,test_mse:164.6.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR(C=1.0,# 与惩罚呈反比\n",
    "              kernel='poly', # linear,poly,rbf,sigmoid,precomputed\n",
    "              degree=3) # poly\n",
    "              # gamma='scale',rbf/poly/sigmoid\n",
    "              # coef0=0.0,poly/sigmoid\n",
    "clf.fit(x_train1,y_train1)\n",
    "EvaluateReg(x_train1,x_test1,y_train1,y_test1,clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_accraucy:0.82,train_auc:0.61,\ntest_accraucy:0.8,test_auc:0.6.\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=1.0,# 与惩罚呈反比\n",
    "              kernel='poly', # linear,poly,rbf,sigmoid,precomputed\n",
    "              degree=3, # poly\n",
    "              # gamma='scale',rbf/poly/sigmoid\n",
    "              # coef0=0.0,poly/sigmoid\n",
    "              decision_function_shape='ovr', # ovo / ovr\n",
    "              random_state=None)\n",
    "clf.fit(x_train2,y_train2)\n",
    "EvaluateClass(x_train2,x_test2,y_train2,y_test2,clf)"
   ]
  },
  {
   "source": [
    "## 4.5 最近邻\n",
    "### 4.5.1 无监督最近邻\n",
    "用于探测数据点的距离关系。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2,\n",
    "                        radius=0.2, # 距离半径\n",
    "                        algorithm='auto', # ball_tree,kd_tree,brute\n",
    "                        leaf_size=30, # BallTree(克服KD树高维失效) / KDTree(维数小于20时效率最高)\n",
    "                        metric='minkowski', # 树距离的度量方法\n",
    "                        p=2, # p=1：曼哈顿距离，p=2，欧氏距离\n",
    "                        n_jobs=-1).fit(X)\n",
    "distances,indices = nbrs.kneighbors(X)\n",
    "nbrs.kneighbors_graph(X).toarray() # 距离相同会按数据顺序选取前k个"
   ]
  },
  {
   "source": [
    "### 4.5.2 有监督最近邻\n",
    "- KNN：较常用，通常较大的 $k$ 会抑制噪声的影响，但使得分类界限不明显。\n",
    "- RadiusNeighborsClassifier：当数据不均匀，基于半径的近邻分类可能是更好的选择；对于高维空间，由于“维度灾难”而不那么有效。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## 4.7 高斯过程\n",
    "## 4.8 横向分解\n",
    "## 4.9 朴素贝叶斯\n",
    "## 4.10 决策树\n",
    "## 4.11 整体方法\n",
    "## 4.12 多类和多输出算法\n",
    "## 4.13 特征选择\n",
    "## 4.14 Semi-supervised学习\n",
    "## 4.15 等张回归\n",
    "## 4.16 概率校准\n",
    "## 4.17 神经网络模型(监督)\n",
    "\n",
    "## 过拟合\n",
    "\n",
    "\n",
    "不同于PCA方差最大化理论，\n",
    "PCA(principal Component Analysis)，即(无监督))，是一种使用最广泛的数据压缩算法。在PCA中，数据从原来的坐标系转换到新的坐标系，由数据本身决定。转换坐标系时，以方差最大的方向作为坐标轴方向，因为数据的最大方差给出了数据的最重要的信息。第一个新坐标轴选择的是原始数据中方差最大的方法，第二个新坐标轴选择的是与第一个新坐标轴正交且方差次大的方向。重复该过程，重复次数为原始数据的特征维数。\n",
    "\n",
    "通过这种方式获得的新的坐标系，我们发现，大部分方差都包含在前面几个坐标轴中，后面的坐标轴所含的方差几乎为0,。于是，我们可以忽略余下的坐标轴，只保留前面的几个含有绝不部分方差的坐标轴。事实上，这样也就相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，也就实现了对数据特征的降维处理。"
   ]
  }
 ]
}